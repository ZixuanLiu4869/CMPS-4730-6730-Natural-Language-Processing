{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6RxqIirisTj"
      },
      "source": [
        "# HW3\n",
        "\n",
        "In this homework, we'll learn about transformers and chatbots.\n",
        "\n",
        "It will probably be easiest to run this on http://colab.research.google.com"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## minGPT Character Language Model\n",
        "\n",
        "First, will inspect Karpathy's [minGPT](https://github.com/karpathy/minGPT/tree/master) library to learn more about transformers.\n",
        "\n",
        "We'll first fit a character language model using mingpt. We'll use as training data all the text of Shakespeare."
      ],
      "metadata": {
        "id": "whaHOZpKj_-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the library\n",
        "!git clone https://github.com/karpathy/minGPT.git"
      ],
      "metadata": {
        "id": "f_q_lNGEjs7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1949576-01aa-4d31-f100-58919d1909cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'minGPT'...\n",
            "remote: Enumerating objects: 489, done.\u001b[K\n",
            "remote: Total 489 (delta 0), reused 0 (delta 0), pack-reused 489\u001b[K\n",
            "Receiving objects: 100% (489/489), 1.44 MiB | 7.26 MiB/s, done.\n",
            "Resolving deltas: 100% (260/260), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add mingpt to your Python path, so you can import it.\n",
        "import sys\n",
        "sys.path.insert(0, './minGPT')\n",
        "from mingpt.model import GPT\n",
        "from mingpt.trainer import Trainer\n",
        "from mingpt.utils import set_seed\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "set_seed(3407)"
      ],
      "metadata": {
        "id": "eDv1T3BNjwzb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download shakespeare data\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "mPHv0dGquVpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c89e4f6-e764-4633-f1f1-0ed5ca3761e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-18 00:22:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-04-18 00:22:28 (22.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading and training code"
      ],
      "metadata": {
        "id": "fOnS25h6iSAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
        "import os\n",
        "import sys\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This represents a dataset of characters.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CN()\n",
        "        C.block_size = 128\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "        self.config = config\n",
        "        self.parse_data(data)\n",
        "\n",
        "    def parse_data(self, data):\n",
        "        print('parsing char data')\n",
        "        # get list of all characters\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        # map from char to int\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        # map from into to char\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.config.block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.config.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # return as tensors\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "def get_config():\n",
        "\n",
        "    C = CN()\n",
        "\n",
        "    # system\n",
        "    C.system = CN()\n",
        "    C.system.seed = 3407\n",
        "    C.system.work_dir = './out'\n",
        "\n",
        "    # data\n",
        "    C.data = CharDataset.get_default_config()\n",
        "\n",
        "    # model\n",
        "    C.model = GPT.get_default_config()\n",
        "    C.model.model_type = 'gpt-micro'\n",
        "\n",
        "    # trainer\n",
        "    C.trainer = Trainer.get_default_config()\n",
        "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "def train_model(config, train_dataset, sample_fn):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    config..........CfgNode\n",
        "    train_dataset...Dataset that emits strings for training\n",
        "    sample_fn.......function to call during training to show sample output.\n",
        "    \"\"\"\n",
        "    # construct the model\n",
        "    config.model.vocab_size = train_dataset.get_vocab_size()\n",
        "    config.model.block_size = train_dataset.get_block_size()\n",
        "    model = GPT(config.model)\n",
        "\n",
        "    # construct the trainer object\n",
        "    trainer = Trainer(config.trainer, model, train_dataset)\n",
        "\n",
        "    # iteration callback\n",
        "    def batch_end_callback(trainer):\n",
        "\n",
        "        if trainer.iter_num % 10 == 0:\n",
        "            print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "\n",
        "        if trainer.iter_num % 500 == 0:\n",
        "            # evaluate both the train and test score\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # sample from the model...\n",
        "                context = list(train_dataset.itos.values())[0]\n",
        "                completion = sample_fn(context, model, trainer, train_dataset, maxlen=100, temperature=1.)\n",
        "                print('sample from the model:')\n",
        "                print(completion)\n",
        "            # save the latest model\n",
        "            print(\"saving model\")\n",
        "            ckpt_path = os.path.join(config.system.work_dir, \"model.pt\")\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            # revert model to training mode\n",
        "            model.train()\n",
        "\n",
        "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "    # run the optimization\n",
        "    trainer.run()\n",
        "    model.eval()\n",
        "    return model, trainer\n",
        "\n",
        "def configure_model(max_iters=100, block_size=128):\n",
        "    config = get_config()\n",
        "    config.merge_from_args(['--trainer.max_iters=%d' % max_iters,\n",
        "                            '--data.block_size=%d' % block_size,\n",
        "                            '--model.block_size=%d' % block_size])\n",
        "    setup_logging(config)\n",
        "    set_seed(config.system.seed)\n",
        "    return config\n",
        "\n",
        "\n",
        "def create_char_data(config):\n",
        "    # construct the training dataset\n",
        "    text = open('input.txt', 'r').read()\n",
        "    return CharDataset(config.data, text)\n",
        "\n",
        "def sample_from_char_model(context, model, trainer, train_dataset, maxlen=500, temperature=1.):\n",
        "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "    y = model.generate(x, maxlen, temperature=temperature, do_sample=True, top_k=10)[0]\n",
        "    return ''.join([train_dataset.itos[int(i)] for i in y])"
      ],
      "metadata": {
        "id": "frsP7S3DuMhT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the character model.\n",
        "config = configure_model(max_iters=100, block_size=64)\n",
        "#config = configure_model(max_iters=100, block_size=64*2)\n",
        "train_dataset = create_char_data(config)\n",
        "model, trainer = train_model(config, train_dataset, sample_from_char_model)"
      ],
      "metadata": {
        "id": "hpp2SypliPPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381082fe-db27-4678-e8e1-ca3dd8ffe301"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command line overwriting config attribute trainer.max_iters with 100\n",
            "command line overwriting config attribute data.block_size with 128\n",
            "command line overwriting config attribute model.block_size with 128\n",
            "parsing char data\n",
            "data has 1115394 characters, 65 unique.\n",
            "number of parameters: 0.82M\n",
            "running on device cuda\n",
            "iter_dt 0.00ms; iter 0: train loss 4.18332\n",
            "sample from the model:\n",
            "\n",
            "trt t  w,the Qe QiQrsv.teeQrs.i srQsr we sihyQ hsw w QihQit ik miktoe, e t tstQiiPosw hw hh, ee,ytsr\n",
            "saving model\n",
            "iter_dt 35.53ms; iter 10: train loss 3.26646\n",
            "iter_dt 37.83ms; iter 20: train loss 2.97872\n",
            "iter_dt 40.16ms; iter 30: train loss 2.79543\n",
            "iter_dt 38.67ms; iter 40: train loss 2.68608\n",
            "iter_dt 36.26ms; iter 50: train loss 2.62622\n",
            "iter_dt 39.98ms; iter 60: train loss 2.63599\n",
            "iter_dt 37.28ms; iter 70: train loss 2.53588\n",
            "iter_dt 38.40ms; iter 80: train loss 2.56386\n",
            "iter_dt 39.06ms; iter 90: train loss 2.51362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_from_char_model(\"Romeo:\", model, trainer, train_dataset, maxlen=10, temperature=1))"
      ],
      "metadata": {
        "id": "ty_WfOozuVN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75530afa-ffd6-4dda-fc67-012670497009"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Romeo: thom myon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the `block_size` variable? Describe in detail what it does.**\n",
        "\n",
        "You might want to consult the code for [model.py](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py).\n",
        "\n"
      ],
      "metadata": {
        "id": "jB30f-tj-uyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The block_size variable in the model configuration determines the maximum length of the sequence of tokens the model can handle in a single forward pass. Each position in the input sequence can attend to positions up to the block_size limit. This means that when processing any token, the model can only use tokens that appear within the preceding block_size positions as context. It also influences the construction of the attention mask in the model."
      ],
      "metadata": {
        "id": "jgMZ82gwi1ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the relationship between `block_size` and the total number of parameters in the model?** That is, if we double `block_size`, what happens to the total number of model parameters?"
      ],
      "metadata": {
        "id": "oppLFY7m_yiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total number of model parameters will slightly increase. If we set block_size=64, the number of parameter is 0.81M. If we set block_size=128, the number of parameter is 0.82M, which is slightly increased. Indeed, the most direct impact of block_size is on the positional embedding layer. The positional embedding matrix has a shape of [block_size, n_embd], where n_embd is the embedding dimensionality. If block_size is doubled, the size of the positional embedding matrix doubles in terms of rows (from [block_size, n_embd] to [2*block_size, n_embd]). This means there will be more parameters in this matrix specifically. However, this increase is relatively small compared to the entire model."
      ],
      "metadata": {
        "id": "zMMlhs6ji9eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the `n_layer` parameter? Describe in detail what it does. If we double this parameter, what happens to the total number of model parameters?**"
      ],
      "metadata": {
        "id": "6tjo4Tyu_zAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The n_layer parameter specifies the number of transformer blocks that make up the model. If we double the n_layer parameter, we double the number of all these components:self-attention modules and feed-forward networks across the model. This results in a near doubling of the total number of parameters."
      ],
      "metadata": {
        "id": "u82DFB4cAhjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does the temperature paramter do?** See the generate method in [model.py](https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L283).\n",
        "\n",
        "Try setting temperature to different values. What do you observe about the output?"
      ],
      "metadata": {
        "id": "r8adX_STRbjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The temperature parameter in the generate method controlls the randomness of the text generation process. It's a scaling factor used in the process of selecting the next token in sequence generation tasks. With higher temperature, the model generates more random and surprising text, as less likely tokens get a relatively higher chance of being selected. While lower temperatures decrease randomness, making the output more predictable and closer to the most likely outcomes. If we set temperature = 1.0, the sampling is based purely on the model's learned probabilities without any additional bias."
      ],
      "metadata": {
        "id": "l_4ffALfRuET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does [line 148](https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L148) in model.py do? How does this relate to the transformer model?**  "
      ],
      "metadata": {
        "id": "IxqGbnN0oXib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]). This line is a list that creates n_layer number of the Block class. Each instance of Block represents a transformer block. It defines the multi-layer structure of the model."
      ],
      "metadata": {
        "id": "gbcrKfRSogw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Model\n",
        "Now, let's fit a word model instead of a character model.\n",
        "\n",
        "Given a string like:\n",
        "\n",
        "> The cow     jumped over the moon. The moon is full tonight!\n",
        "\n",
        "The `WordDataset` class below should create tokens for each space-delimited string:\n",
        "\n",
        "> ['The', 'cow', 'jumped', 'over', 'the', 'moon', '.', 'The', 'moon', 'is', 'full', 'tonight', '!']\n",
        "\n",
        "Note that multiple space characters are treated as one (Hint: `re` may help here.)\n",
        "\n",
        "Using `CharDataset::parse_data` function above as an example, complete the `parse_data` function below to set the `stoi`, `itos`, `vocab_size`, and `data` attributes of the `WordDataset` class."
      ],
      "metadata": {
        "id": "ijvSHpxC9dtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class WordDataset(CharDataset):\n",
        "  def parse_data(self, data):\n",
        "    \"\"\"\n",
        "    data.....A single string representing many sentences.\n",
        "    \"\"\"\n",
        "    # Use regex to split words and punctuation\n",
        "    tokens = re.findall(r'\\w+|[^\\w\\s]', data)\n",
        "\n",
        "    # Calculate unique tokens and their frequencies\n",
        "    unique_tokens = sorted(set(tokens))\n",
        "    vocab_size = len(unique_tokens)\n",
        "    print('Data has %d tokens, %d unique.' % (len(tokens), vocab_size))\n",
        "\n",
        "    # Create mappings from words to indices and indices to words\n",
        "    self.stoi = {token: i for i, token in enumerate(unique_tokens)}\n",
        "    self.itos = {i: token for i, token in enumerate(unique_tokens)}\n",
        "    self.vocab_size = vocab_size\n",
        "    self.data = tokens\n",
        "\n",
        "word_config = configure_model(max_iters=200, block_size=4)\n",
        "word_data = WordDataset(word_config.data, 'The cow jumped over the moon. The moon is full tonight!')\n",
        "word_data.data"
      ],
      "metadata": {
        "id": "u7Lyti4f7jdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445e6b80-5079-43f5-8a52-0ed7794fb15b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command line overwriting config attribute trainer.max_iters with 200\n",
            "command line overwriting config attribute data.block_size with 4\n",
            "command line overwriting config attribute model.block_size with 4\n",
            "Data has 13 tokens, 11 unique.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'cow',\n",
              " 'jumped',\n",
              " 'over',\n",
              " 'the',\n",
              " 'moon',\n",
              " '.',\n",
              " 'The',\n",
              " 'moon',\n",
              " 'is',\n",
              " 'full',\n",
              " 'tonight',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now reuse the training code to fit the word language model.\n",
        "def sample_from_word_model(context, model, trainer, train_dataset, maxlen=500, temperature=1.):\n",
        "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "    y = model.generate(x, maxlen, temperature=temperature, do_sample=True, top_k=10)[0]\n",
        "    return ' '.join([train_dataset.itos[int(i)] for i in y])\n",
        "\n",
        "word_model, word_trainer = train_model(word_config, word_data, sample_from_word_model)"
      ],
      "metadata": {
        "id": "ftCK_2L8-uHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "951b6a55-050a-41c4-b482-f4b250c025c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 0.80M\n",
            "running on device cuda\n",
            "iter_dt 0.00ms; iter 0: train loss 2.42279\n",
            "sample from the model:\n",
            "! jumped The moon moon is full jumped moon tonight moon full tonight ! tonight full The is is moon is tonight The moon . The moon is The is full full full is full tonight ! . . full . full is moon moon The moon moon is jumped is The ! The tonight full is . moon is full is ! The tonight . moon moon tonight full The the moon The . moon jumped ! over moon the over moon moon The moon is ! is ! moon moon tonight . . the moon . full is .\n",
            "saving model\n",
            "iter_dt 13.65ms; iter 10: train loss 0.63155\n",
            "iter_dt 14.08ms; iter 20: train loss 0.39190\n",
            "iter_dt 13.19ms; iter 30: train loss 0.24487\n",
            "iter_dt 14.01ms; iter 40: train loss 0.16866\n",
            "iter_dt 14.68ms; iter 50: train loss 0.15440\n",
            "iter_dt 13.45ms; iter 60: train loss 0.12987\n",
            "iter_dt 13.91ms; iter 70: train loss 0.10922\n",
            "iter_dt 13.65ms; iter 80: train loss 0.07356\n",
            "iter_dt 13.22ms; iter 90: train loss 0.07683\n",
            "iter_dt 13.25ms; iter 100: train loss 0.09613\n",
            "iter_dt 18.14ms; iter 110: train loss 0.08248\n",
            "iter_dt 13.55ms; iter 120: train loss 0.09189\n",
            "iter_dt 13.37ms; iter 130: train loss 0.08596\n",
            "iter_dt 18.38ms; iter 140: train loss 0.08484\n",
            "iter_dt 13.24ms; iter 150: train loss 0.09598\n",
            "iter_dt 15.28ms; iter 160: train loss 0.09519\n",
            "iter_dt 13.21ms; iter 170: train loss 0.08792\n",
            "iter_dt 13.25ms; iter 180: train loss 0.07219\n",
            "iter_dt 13.36ms; iter 190: train loss 0.07377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_from_word_model([\"The\"], word_model, word_trainer, word_data, maxlen=50, temperature=1.)"
      ],
      "metadata": {
        "id": "fZweO3GBIKsM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1bbe6b3c-992d-44a3-e2a9-6dfe44ec9f10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The moon is full tonight ! ! ! full tonight ! ! ! jumped over the moon . The moon is full tonight ! ! ! ! ! full tonight ! ! ! . The moon is full tonight ! ! ! full tonight ! ! ! full tonight ! !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia\n",
        "\n",
        "With our word model, let's now fit a language model on the Wikipedia page for [New Orleans](https://en.wikipedia.org/wiki/New_Orleans)\n",
        "\n",
        "First, we'll install a library to help us fetch the plain text of a wikipedia page."
      ],
      "metadata": {
        "id": "-lYiFOD3MVhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "RnhwHz7-McWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c04d3f4-2477-4007-91c6-0f52600aca50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=cf9641e84cd1ea2360cfb7a68d25bd4f8253085590330845eeab8c9fb18e4820\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "wikipedia.set_lang('en')\n",
        "page = wikipedia.page('New Orleans')\n",
        "print(page.content[:100])"
      ],
      "metadata": {
        "id": "A3pclbQqL2zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d7c487-0b60-48b8-d699-d56ae2f47cd7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Orleans (commonly known as NOLA or the Big Easy among other nicknames) is a consolidated city-pa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create new variables `word_config`, `word_data`, `word_model`, `word_trainer` that are analogous to the ones used previously. These should fit a model to the `page` text defined in the previous cell.**"
      ],
      "metadata": {
        "id": "6OB56CJpkKKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config():\n",
        "\n",
        "    C = CN()\n",
        "\n",
        "    # system\n",
        "    C.system = CN()\n",
        "    C.system.seed = 3407\n",
        "    C.system.work_dir = './out'\n",
        "\n",
        "    # data\n",
        "    C.data = CharDataset.get_default_config()\n",
        "\n",
        "    # model\n",
        "    C.model = GPT.get_default_config()\n",
        "    C.model.model_type = 'gpt-micro'\n",
        "    #C.model.n_layer = None\n",
        "    #C.model.n_embd =  None\n",
        "\n",
        "    # trainer\n",
        "    C.trainer = Trainer.get_default_config()\n",
        "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
        "\n",
        "    return C\n",
        "\n",
        "def configure_model(max_iters=100, block_size=128):\n",
        "    config = get_config()\n",
        "    config.merge_from_args(['--trainer.max_iters=%d' % max_iters,\n",
        "                            '--data.block_size=%d' % block_size,\n",
        "                            '--model.block_size=%d' % block_size])\n",
        "    setup_logging(config)\n",
        "    set_seed(config.system.seed)\n",
        "    return config\n",
        "\n",
        "config = configure_model(max_iters=100, block_size=64)\n",
        "word_data= WordDataset(config.data, page.content)\n",
        "word_model, word_trainer = train_model(config, word_data, sample_from_word_model)\n"
      ],
      "metadata": {
        "id": "jVcV90oKMjaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5372f7-d70b-444b-ec72-715bb4f3102e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command line overwriting config attribute trainer.max_iters with 100\n",
            "command line overwriting config attribute data.block_size with 64\n",
            "command line overwriting config attribute model.block_size with 64\n",
            "Data has 20377 tokens, 4270 unique.\n",
            "number of parameters: 1.35M\n",
            "running on device cuda\n",
            "iter_dt 0.00ms; iter 0: train loss 8.38579\n",
            "sample from the model:\n",
            "\" best escaped recorded Archive the expand many break COVID resumption , the many 120 , being many 120 the Bouligny of resumption Upper while Improvising of Exchange categories Democratic gender the the percent of Nagin Improvising turned categories railroads many the established for , 120 office the the of being , 151 It 120 for the southern , order peace libres in the Loyola 6 says 6 expand 6 1792 escaped 1792 1731 housing 120 many 120 Army Appalachian of Luther break , break says decay 120 Archive Navtech of part bypass it housing factors jobs jobs doughnuts bypass of\n",
            "saving model\n",
            "iter_dt 25.98ms; iter 10: train loss 7.58332\n",
            "iter_dt 24.16ms; iter 20: train loss 7.07566\n",
            "iter_dt 25.64ms; iter 30: train loss 6.67191\n",
            "iter_dt 23.05ms; iter 40: train loss 6.33196\n",
            "iter_dt 23.85ms; iter 50: train loss 5.99570\n",
            "iter_dt 25.55ms; iter 60: train loss 5.72702\n",
            "iter_dt 21.11ms; iter 70: train loss 5.48919\n",
            "iter_dt 17.98ms; iter 80: train loss 5.33205\n",
            "iter_dt 21.84ms; iter 90: train loss 5.20786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_from_word_model([\"A\", \"local\", \"variant\", \"for\", \"hip\", \"hop\", \"is\"], word_model, word_trainer, word_data, maxlen=200, temperature=1.)"
      ],
      "metadata": {
        "id": "cg1v3nK2QKi-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "bb1273c4-ad92-40da-ef40-f42760e82797"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A local variant for hip hop is been a to the most of the city and New Orleans from the most of the most of the French Quarter to a significant to the city from of a with the city \\' s city , the city in New Orleans was the U . In the city to the United States . The the city . = = = = = = = = In the French , and the city was a \" . New Orleans . S . According to the French and a to the city \\' s largest to the Mississippi , 000 in the world - Katrina , and its s city is Louisiana in the United States . The city in the Mississippi River , the French in the United States ) . = In other of the city \\' s city , which . The city \\' s , the city as the nation \\' Orleans \\' s United States . = = = The city was the city \\' s first in the city of the world . S . A \" , and the city in a term in the first in the city \\' s population of New'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Investigate different model settings (`block_size, max_iters, learning_rate, n_embd, n_layer`).\n",
        "\n",
        "**What effect do you notice from trying different values? Which setting appears to generate the best generated text?**\n"
      ],
      "metadata": {
        "id": "LjKzZ1gHglDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we don't change the model type, where the n_embd and n_layer is default.\n",
        "\n",
        "\n",
        "If the block_size=64, max_iters=100, learning_rate=5e-4. Final loss is 5.21\n",
        "\n",
        "If the block_size=64, max_iters=200, learning_rate=5e-4. Final loss is 4.03. The sample for the text is like \"A local variant for hip hop is , and African American , New Orleans had been also known . The city had a study by the city of over it East . The New Orleans is the first - largest city ' s population . S . The first to this area is home to the Mississippi River and its - Katrina in the city ' s population . S . In 2010 , and other cities in Louisiana is the most of New Orleans ' s murder rate of New Orleans was the city had been Orleans is the first - Katrina was also ) , with a study by the city . A population . The city and African American . In January that had been Orleans had the National Orleans ' s office , and its own , and Latino Americans , the French Quarter ( all of the National Historical , 000 cities in New Orleans is a historic peak of Louisiana , the New Orleans was in the United States , New Orleans ' major bridge - Katrina - Catholic York , and other parishes . After the nation ' s largest population . = = = = = = = =\" It much better than the first one. Increase the max iteration will increase the performance.\n",
        "\n",
        "\n",
        "If the block_size=64*4, max_iters=100, learning_rate=5e-4. Final loss is 4.81265. It can somehow increase the performance.\n",
        "\n",
        "The change of the learning_rate doesn't seem to increase the performance. The setting that has larger block_size and max_iters can yeild better performance.\n",
        "\n",
        "As for n_embd and n_layer, increase them seems to increse the performance.\n"
      ],
      "metadata": {
        "id": "mHxKPzVag3E4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you wanted to take the word model trained on the New Orleans Wikipedia page and use supervised fine-tuning to create a chatbot that answers questions about New Orleans.\n",
        "\n",
        "**What type of additional training data would you need to do this?**\n",
        "\n",
        "Provide example data below."
      ],
      "metadata": {
        "id": "jkacGUNjVwhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fine-tune a word model trained on the New Orleans Wikipedia page into a chatbot capable of answering questions about New Orleans, we would need a dataset comprising pairs of questions and answers related to the topic. The questions would simulate potential queries a user might pose about New Orleans, and the answers would provide the appropriate responses based on factual information. For example:\n",
        "Q: What is New Orleans known for?\n",
        "A: New Orleans is renowned for its distinctive music, Creole cuisine, unique dialects, and its annual celebrations and festivals, most notably Mardi Gras.\n",
        "\n"
      ],
      "metadata": {
        "id": "QPCc4ZlngSTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If this new data contains words that don't appear in the New Orleans wikipedia page, what will happen? How can you fix this?**"
      ],
      "metadata": {
        "id": "qEM97S3xgTjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the new dataset for training the chatbot contains words that do not appear in the original New Orleans Wikipedia page, those words will be unrecognized by the model because they are not present in the vocabulary (stoi mapping) used during the initial training. It will cause the unknown token problem. The model won't have embeddings or learned parameters for these out-of-vocabulary words, leading to potential errors. To fix that, we can use pre-trained word embeddings that cover a broader vocabulary. These embeddings can be fine-tuned along with the model or used as a fixed input layer."
      ],
      "metadata": {
        "id": "ZcO_yJkxgdzf"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}